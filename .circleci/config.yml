version: 2.1

orbs:
  docker: circleci/docker@2.2.0

executors:
  node:
    docker:
      - image: circleci/node:13.8.0 
  python-buster:
    docker:
      - image: python:3.9-buster
  python-alpine:
    docker:
      - image: python:3.7-alpine3.11
      #python:3.9-alpine
  aws:
    docker:
      - image: amazon/aws-cli:latest


commands:

  destroy-environment:
    description: Destroy cloudformation stacks given a workflow ID.
    parameters:
      Workflow_ID:
        type: string
        default: ${CIRCLE_WORKFLOW_ID:0:7}  
    steps:
      - run:
          name: Destroy environments
          when: on_fail
          command: |
            echo "Destroying environment nodes"
            aws cloudformation delete-stack \
              --stack-name "${ENVIRONMENT_NAME}-nodes"

            echo "Destroying environment cluster"
            aws cloudformation delete-stack \
              --stack-name "${ENVIRONMENT_NAME}-cluster"

            echo "Destroying environment network"
            aws cloudformation delete-stack \
              --stack-name "${ENVIRONMENT_NAME}-network"

jobs:
 
  test-app:
    executor: python-buster
    working_directory: ~/repo

    steps:
      - checkout
      # Download and cache dependencies
      - restore_cache:
          keys:
            - v1-dependencies-{{ checksum "requirements.txt" }}
            # fallback to using the latest cache if no exact match is found
            - v1-dependencies-

      - run:
          name: install dependencies
          command: |
            python3 -m venv venv
            . venv/bin/activate
            make install
            # Install hadolint
            wget -O /bin/hadolint https://github.com/hadolint/hadolint/releases/download/v1.16.3/hadolint-Linux-x86_64 &&\
            chmod +x /bin/hadolint

      - save_cache:
          paths:
            - ./venv
          key: v1-dependencies-{{ checksum "requirements.txt" }}

      # run lint!
      - run:
          name: run lint
          command: |
            . venv/bin/activate
            make lint

  


  build-image:
    docker:
      - image: cimg/go:1.17
    steps:
      - checkout

      - setup_remote_docker:
          version: 20.10.14
          docker_layer_caching: true

      - run:
          name: Build Docker image
          command: |
            echo "Docker ID and Image: $DOCKER_PATH"

            docker build --tag=valencia-real-estate-report .

            # Authenticate & tag
            docker login -u $DOCKER_USERNAME -p $DOCKER_PASSWORD
            docker tag valencia-real-estate-report $DOCKER_PATH:latest

            # Push image to a docker repository
            docker push $DOCKER_PATH:latest
            

  deploy-network:
    executor: aws
    steps:
      - checkout

      - run:
          name: Deploy EKS network
          command: |
            aws cloudformation deploy \
              --stack-name ${ENVIRONMENT_NAME}-network \
              --template-file infrastructure/eks-network.yml \
              --parameter-overrides file://infrastructure/eks-network-parameters.json \
              --capabilities "CAPABILITY_IAM" "CAPABILITY_NAMED_IAM" \
              --region $AWS_DEFAULT_REGION \
              --tags project=${ENVIRONMENT_NAME}-project 


  deploy-cluster:
    executor: aws
    steps:
      - checkout
      - run:
          name: Deploy EKS cluster
          no_output_timeout: 30m
          command: |
            aws cloudformation deploy \
              --stack-name ${ENVIRONMENT_NAME}-cluster \
              --template-file infrastructure/eks-cluster.yml \
              --parameter-overrides file://infrastructure/eks-cluster-parameters.json \
              --capabilities "CAPABILITY_IAM" "CAPABILITY_NAMED_IAM" \
              --region $AWS_DEFAULT_REGION \
              --tags project=${ENVIRONMENT_NAME}-project 

  deploy-nodes:
    executor: aws
    steps:
      - checkout

      - run:
          name: Install dependencies for persisting workspace
          command: yum install -y tar gzip

      - run:
          name: Deploy EKS nodes
          command: |

            aws cloudformation deploy \
              --stack-name ${ENVIRONMENT_NAME}-nodes \
              --template-file infrastructure/eks-nodes.yml \
              --parameter-overrides file://infrastructure/eks-nodes-parameters.json \
              --capabilities "CAPABILITY_IAM" "CAPABILITY_NAMED_IAM" \
              --region $AWS_DEFAULT_REGION \
              --tags project=${ENVIRONMENT_NAME}-project \
              --output text >> ./ansible/server-status.txt

            cat ./ansible/server-status.txt

      - run:
          name: Get  IPs for Ansible
          command: |
            aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=${ENVIRONMENT_NAME}-JumpHost" \
              --query 'Reservations[*].Instances[*].PublicIpAddress' \
              --output text >> ./ansible/inventory.txt

            cat ./ansible/inventory.txt

      - persist_to_workspace:
          root: .
          paths:
            - ./ansible/inventory.txt
            - ./ansible/server-status.txt

      #- destroy-environment

  configure-jumphost:
    executor: python-alpine
    steps:

      - checkout

      - attach_workspace:
          at: .

      - add_ssh_keys:
          fingerprints: ["be:f8:9f:b2:30:c9:5e:1e:5e:6a:80:99:19:9c:3e:dd"]

      - run:
          name: Install dependencies
          command: |
            apk add curl
            apk add --upgrade bash
            apk add --update ansible
            apk add openssh-client
            pip3 install awscli

      - run:
          name: Configure Jump Host
          command: | 
            cat ./ansible/inventory.txt

            if grep -q "Successfully created/updated stack" ./ansible/server-status.txt

              then
                echo "Jump Host will be configured."
                export ANSIBLE_HOST_KEY_CHECKING=False
                ansible-playbook -i ./ansible/inventory.txt ./ansible/configure-server.yml   
                
              else
                echo "Jump Host already configured."
              
              fi

      - destroy-environment

  deploy-load-balancer:
    executor: python-alpine
    steps:

      - checkout

      - attach_workspace:
          at: .

      - add_ssh_keys:
          fingerprints: ["be:f8:9f:b2:30:c9:5e:1e:5e:6a:80:99:19:9c:3e:dd"]

      - run:
          name: Install dependencies
          command: |
            apk add curl
            apk add --upgrade bash
            apk add --update ansible
            apk add openssh-client
            pip3 install awscli
            
      - run:
          name: Configure server
          command: |
            if grep -q "Successfully created/updated stack" ./ansible/server-status.txt

              then
                echo "Elastic Load Balancer Service by Kubernetes will be initialized"
                export ANSIBLE_HOST_KEY_CHECKING=False
                ansible-playbook -i ./ansible/inventory.txt ./ansible/deploy-load-balancer.yml   
                sleep 60
                
              else
                echo "Elastic Load Balancer Service already active."
              
              fi

      - run:
          name: Save Elastic Load Balancer DNS
          command: |

            export ANSIBLE_HOST_KEY_CHECKING=False
            ansible-playbook -i ./ansible/inventory.txt ./ansible/get-load-balancer-dns.yml
            cat ./ansible/dns.txt

      - persist_to_workspace:
          root: .
          paths:
            - ./ansible/dns.txt

  deploy-app:
    executor: python-alpine
    steps:

      - checkout

      - attach_workspace:
          at: .

      - add_ssh_keys:
          fingerprints: ["be:f8:9f:b2:30:c9:5e:1e:5e:6a:80:99:19:9c:3e:dd"]

      - run:
          name: Install dependencies
          command: |
            apk add curl
            apk add --upgrade bash
            apk add --update ansible
            apk add openssh-client
            pip3 install awscli

      - run:
          name: Deploy app image to cluster
          command: |
            if grep -q "Successfully created/updated stack" ./ansible/server-status.txt

              then
                echo "App will be deployed to Cluster."
                export ANSIBLE_HOST_KEY_CHECKING=False
                ansible-playbook -i ./ansible/inventory.txt ./ansible/deploy-app.yml  
                
              else
                echo "App already deployed." 
              
              fi
          
      - destroy-environment


  update-app-image:
    executor: python-alpine
    steps:

      - checkout

      - attach_workspace:
          at: .

      - add_ssh_keys:
          fingerprints: ["be:f8:9f:b2:30:c9:5e:1e:5e:6a:80:99:19:9c:3e:dd"]

      - run:
          name: Install dependencies
          command: |
            apk add curl
            apk add --upgrade bash
            apk add --update ansible
            apk add openssh-client
            pip3 install awscli

      - run:
          name: Update docker image
          command: |
            export ANSIBLE_HOST_KEY_CHECKING=False
            ansible-playbook -i ./ansible/inventory.txt ./ansible/update-app.yml  
        
workflows:
  default: 

    jobs:
      - test-app
      - build-image:
          requires: [test-app]
      - deploy-network
      - deploy-cluster:
          requires: [deploy-network]
      - deploy-nodes:
          requires: [deploy-cluster]
      - configure-jumphost:
          requires: [deploy-nodes]
      - deploy-load-balancer:
          requires: [configure-jumphost]
      - deploy-app:
          requires: [build-image, deploy-load-balancer]
      - update-app-image:
          requires: [deploy-app]

